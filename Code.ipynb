{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "sem_seg_custom_dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Atik-Ahamed/Semantic-Segmentation-NODE/blob/master/sem_seg_custom_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEym_MU-H8YL",
        "colab_type": "text"
      },
      "source": [
        "# Importing Necessary file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcMqnYDRiVF3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn\n",
        "import math\n",
        "import numpy as np\n",
        "from IPython.display import clear_output\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import pickle\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "sns.color_palette(\"bright\")\n",
        "import matplotlib as mpl\n",
        "import matplotlib.cm as cm\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch import nn\n",
        "from torch.nn  import functional as F\n",
        "from torch.autograd import Variable\n",
        "from torchvision import models\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "import cv2\n",
        "import random\n",
        "import os\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "import torchvision.transforms.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, datasets, models\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0HLp8O3ICmy",
        "colab_type": "text"
      },
      "source": [
        "# downloading from git"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "easMU2LnIXOV",
        "colab_type": "code",
        "outputId": "39f14ab9-4041-4f28-c522-8d58de73f066",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!git clone https://github.com/usuyama/pytorch-unet.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4eInPa-If8A",
        "colab_type": "code",
        "outputId": "653c1a52-a736-4784-8505-903bc58bd211",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "!mv  -v /content/pytorch-unet/* /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gzfp31eYI7cZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r /content/pytorch-unet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHUEZ6owIIVT",
        "colab_type": "text"
      },
      "source": [
        "# generating dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlC3eOu4M6DB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_shape=(192,192,3)\n",
        "#here in the pytorch model channel first is used it is changed as required in the model definition\n",
        "#but to convention here the channel last order is used as input shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NAziBnQJOhq",
        "colab_type": "code",
        "outputId": "e936e10d-73bd-4c38-e501-9aa811cdd9e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 772
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import helper\n",
        "import simulation\n",
        "\n",
        "# Generate some random images\n",
        "input_images, target_masks = simulation.generate_random_data(input_shape[0], input_shape[1], count=3)\n",
        "\n",
        "for x in [input_images, target_masks]:\n",
        "    print(x.shape)\n",
        "    print(x.min(), x.max())\n",
        "\n",
        "# Change channel-order and make 3 channels for matplot\n",
        "input_images_rgb = [x.astype(np.uint8) for x in input_images]\n",
        "\n",
        "# Map each channel (i.e. class) to each color\n",
        "target_masks_rgb = [helper.masks_to_colorimg(x) for x in target_masks]\n",
        "\n",
        "# Left: Input image (black and white), Right: Target mask (6ch)\n",
        "helper.plot_side_by_side([input_images_rgb, target_masks_rgb])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgSO9yuLPxij",
        "colab_type": "text"
      },
      "source": [
        "# Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjtkhwQCJTKS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimDataset(Dataset):\n",
        "    def __init__(self, count, transform=None):\n",
        "        #here i need extra param as length and type as train or val\n",
        "        self.input_images, self.target_masks = simulation.generate_random_data(input_shape[0], input_shape[1], count=count)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        #return self.length\n",
        "        return len(self.input_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      #here i need to make change\n",
        "        #get_data if train or val\n",
        "        image = self.input_images[idx]\n",
        "        mask = self.target_masks[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return [image, mask]\n",
        "\n",
        "# use the same transformations for train/val in this example\n",
        "trans = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # imagenet\n",
        "])\n",
        "\n",
        "train_set = SimDataset(500, transform = trans)\n",
        "val_set = SimDataset(50, transform = trans)\n",
        "\n",
        "image_datasets = {\n",
        "    'train': train_set, 'val': val_set\n",
        "}\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "dataloaders = {\n",
        "    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n",
        "    'val': DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxKs507cJVbb",
        "colab_type": "code",
        "outputId": "bdfd4a16-8f9c-40ef-a0e3-9b83a5fab040",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "import torchvision.utils\n",
        "\n",
        "def reverse_transform(inp):\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    inp = (inp * 255).astype(np.uint8)\n",
        "\n",
        "    return inp\n",
        "\n",
        "# Get a batch of training data\n",
        "inputs, masks = next(iter(dataloaders['train']))\n",
        "\n",
        "print(inputs.shape, masks.shape)\n",
        "\n",
        "plt.imshow(reverse_transform(inputs[3]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d6Gkes-IOhL",
        "colab_type": "text"
      },
      "source": [
        "# ResNET UNET author's model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B39eVDgiJZwC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "def convrelu(in_channels, out_channels, kernel, padding):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n",
        "        nn.ReLU(inplace=True),\n",
        "    )\n",
        "\n",
        "\n",
        "class ResNetUNet(nn.Module):\n",
        "    def __init__(self, n_class):\n",
        "        super().__init__()\n",
        "\n",
        "        self.base_model = models.resnet18(pretrained=False)\n",
        "        self.base_layers = list(self.base_model.children())\n",
        "\n",
        "        self.layer0 = nn.Sequential(*self.base_layers[:3]) # size=(N, 64, x.H/2, x.W/2)\n",
        "        self.layer0_1x1 = convrelu(64, 64, 1, 0)\n",
        "        self.layer1 = nn.Sequential(*self.base_layers[3:5]) # size=(N, 64, x.H/4, x.W/4)\n",
        "        self.layer1_1x1 = convrelu(64, 64, 1, 0)\n",
        "        self.layer2 = self.base_layers[5]  # size=(N, 128, x.H/8, x.W/8)\n",
        "        self.layer2_1x1 = convrelu(128, 128, 1, 0)\n",
        "        self.layer3 = self.base_layers[6]  # size=(N, 256, x.H/16, x.W/16)\n",
        "        self.layer3_1x1 = convrelu(256, 256, 1, 0)\n",
        "        self.layer4 = self.base_layers[7]  # size=(N, 512, x.H/32, x.W/32)\n",
        "        self.layer4_1x1 = convrelu(512, 512, 1, 0)\n",
        "\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "\n",
        "        self.conv_up3 = convrelu(256 + 512, 512, 3, 1)\n",
        "        self.conv_up2 = convrelu(128 + 512, 256, 3, 1)\n",
        "        self.conv_up1 = convrelu(64 + 256, 256, 3, 1)\n",
        "        self.conv_up0 = convrelu(64 + 256, 128, 3, 1)\n",
        "\n",
        "        self.conv_original_size0 = convrelu(3, 64, 3, 1)\n",
        "        self.conv_original_size1 = convrelu(64, 64, 3, 1)\n",
        "        self.conv_original_size2 = convrelu(64 + 128, 64, 3, 1)\n",
        "\n",
        "        self.conv_last = nn.Conv2d(64, n_class, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x_original = self.conv_original_size0(input)\n",
        "        x_original = self.conv_original_size1(x_original)\n",
        "\n",
        "        layer0 = self.layer0(input)\n",
        "        layer1 = self.layer1(layer0)\n",
        "        layer2 = self.layer2(layer1)\n",
        "        layer3 = self.layer3(layer2)\n",
        "        layer4 = self.layer4(layer3)\n",
        "\n",
        "        layer4 = self.layer4_1x1(layer4)\n",
        "        x = self.upsample(layer4)\n",
        "        layer3 = self.layer3_1x1(layer3)\n",
        "        x = torch.cat([x, layer3], dim=1)\n",
        "        x = self.conv_up3(x)\n",
        "\n",
        "        x = self.upsample(x)\n",
        "        layer2 = self.layer2_1x1(layer2)\n",
        "        x = torch.cat([x, layer2], dim=1)\n",
        "        x = self.conv_up2(x)\n",
        "\n",
        "        x = self.upsample(x)\n",
        "        layer1 = self.layer1_1x1(layer1)\n",
        "        x = torch.cat([x, layer1], dim=1)\n",
        "        x = self.conv_up1(x)\n",
        "\n",
        "        x = self.upsample(x)\n",
        "        layer0 = self.layer0_1x1(layer0)\n",
        "        x = torch.cat([x, layer0], dim=1)\n",
        "        x = self.conv_up0(x)\n",
        "\n",
        "        x = self.upsample(x)\n",
        "        x = torch.cat([x, x_original], dim=1)\n",
        "        x = self.conv_original_size2(x)\n",
        "\n",
        "        out = self.conv_last(x)\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogahYkGJJcfC",
        "colab_type": "code",
        "outputId": "740a4a4a-4e28-4ca1-f8a5-17537efa38c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = ResNetUNet(n_class=6)\n",
        "model = model.to(device)\n",
        "\n",
        "# check keras-like model summary using torchsummary\n",
        "from torchsummary import summary\n",
        "summary(model, input_size=(3, 192, 192))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hm-EcWK73fn",
        "colab_type": "text"
      },
      "source": [
        "# Training Resnet Author's model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BGcML8v78yZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e07e358a-83d3-4e19-b93c-a85e0804e125"
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import time\n",
        "import copy\n",
        "\n",
        "from collections import defaultdict\n",
        "import torch.nn.functional as F\n",
        "from loss import dice_loss\n",
        "loss1=nn.CrossEntropyLoss()\n",
        "sig=nn.Sigmoid()\n",
        "\n",
        "def calc_metrics(pred, target, metrics, bce_weight=0.5):\n",
        "    bce = F.binary_cross_entropy_with_logits(pred, target)\n",
        "\n",
        "    pred = F.sigmoid(pred)\n",
        "    dice = dice_loss(pred, target)\n",
        "\n",
        "    loss = bce * bce_weight + dice * (1 - bce_weight)\n",
        "\n",
        "    numerator =  (pred * target).sum((1, 2, 3))\n",
        "    denominator = (pred + target).sum((1, 2, 3))\n",
        "\n",
        "    iou=(numerator / (denominator-numerator)).mean()\n",
        "\n",
        "\n",
        "    # pred1=pred.data.cpu().numpy()\n",
        "    # target1=target.data.cpu().numpy()\n",
        "    # pred1=np.argmax(pred1,axis=1)\n",
        "    # target1=np.argmax(target1,axis=1)\n",
        "    # #print(pred1.shape)\n",
        "    # pred1=np.reshape(pred1,(pred1.shape[0]*pred1.shape[1]*pred1.shape[2]))\n",
        "    # target1=np.reshape(target1,(target1.shape[0]*target1.shape[1]*target1.shape[2]))\n",
        "    # same=0\n",
        "    # #print(pred.shape)\n",
        "    # for i in range(0,pred1.shape[0]):\n",
        "    #   if pred1[i]==target1[i]:\n",
        "    #     same=same+1\n",
        "    # acc=same/pred1.shape[0]\n",
        "    #print(acc)\n",
        "\n",
        "    # _, preds = torch.max(pred, dim=1)\n",
        "    # _, targets = torch.max(target, dim=1)\n",
        "    # valid = (targets >= 0).long()\n",
        "    # acc_sum = torch.sum(valid * (preds == targets).long())\n",
        "    # pixel_sum = torch.sum(valid)\n",
        "    # acc = acc_sum.float() / (pixel_sum.float() + 1e-10)\n",
        "\n",
        "\n",
        "    #acc=(torch.sum((torch.argmax(pred,dim=1)==torch.argmax(target,dim=1))).item())/(target.shape[0]*target.shape[1]*target.shape[2]*target.shape[3])\n",
        "    #print(acc)\n",
        "\n",
        "    #print(target.size(0))\n",
        "    metrics['bce'] += bce.data.cpu().numpy() * target.size(0)\n",
        "    metrics['dice'] += dice.data.cpu().numpy() * target.size(0)\n",
        "    metrics['loss'] += loss.data.cpu().numpy() * target.size(0)\n",
        "    metrics['iou'] += iou.data.cpu().numpy() * target.size(0)\n",
        "   # metrics['acc'] +=acc* target.size(0)\n",
        "\n",
        "    return loss\n",
        "\n",
        " \n",
        "def print_metrics(metrics, epoch_samples, phase):\n",
        "    outputs = []\n",
        "    for k in metrics.keys():\n",
        "        outputs.append(\"{}: {:4f}\".format(k, metrics[k] / epoch_samples))\n",
        "\n",
        "    print(\"{}: {}\".format(phase, \", \".join(outputs)))\n",
        "\n",
        "def train_model(model, optimizer, scheduler, num_epochs=25):\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = 1e10\n",
        "    best_iou=-1\n",
        "    best_dice=1e10\n",
        "    best_bce=1e10\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        since = time.time()\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "                for param_group in optimizer.param_groups:\n",
        "                    print(\"LR\", param_group['lr'])\n",
        "\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            metrics = defaultdict(float)\n",
        "            epoch_samples = 0\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                #print(inputs.shape,labels.shape)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    #print(outputs.shape)\n",
        "                    loss = calc_metrics(outputs, labels, metrics)\n",
        "                    #print(loss.item())\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                epoch_samples += inputs.size(0)\n",
        "\n",
        "            print_metrics(metrics, epoch_samples, phase)\n",
        "            epoch_loss = metrics['loss'] / epoch_samples\n",
        "            epoch_dice = metrics['dice'] / epoch_samples\n",
        "            epoch_bce = metrics['bce'] / epoch_samples\n",
        "            epoch_iou = metrics['iou'] / epoch_samples\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val':\n",
        "              if epoch_loss < best_loss:\n",
        "                print(\"saving best model\")\n",
        "                best_loss = epoch_loss\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "              if epoch_dice<best_dice:\n",
        "                best_dice=epoch_dice\n",
        "              if epoch_bce<best_bce:\n",
        "                best_bce=epoch_bce\n",
        "              if epoch_iou>best_iou:\n",
        "                best_iou=epoch_iou\n",
        "\n",
        "        time_elapsed = time.time() - since\n",
        "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "\n",
        "    print('Best val loss: {:4f}'.format(best_loss))\n",
        "    print('Best val dice: {:4f}'.format(best_dice))\n",
        "    print('Best val bce: {:4f}'.format(best_bce))\n",
        "    print('Best val iou: {:4f}'.format(best_iou))\n",
        "     # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "optimizer_ft = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
        "\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=200, gamma=0.1)\n",
        "\n",
        "model = train_model(model, optimizer_ft, exp_lr_scheduler, num_epochs=500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9FRuSTrIV5S",
        "colab_type": "text"
      },
      "source": [
        "# ODE model our created model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpFVb9E94eoE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ode_solve(z0, t0, t1, f):\n",
        "    \"\"\"\n",
        "    Simplest Euler ODE initial value solver\n",
        "    \"\"\"\n",
        "    h_max = 0.05\n",
        "    n_steps = math.ceil((abs(t1 - t0)/h_max).max().item())\n",
        "\n",
        "    h = (t1 - t0)/n_steps\n",
        "    t = t0\n",
        "    z = z0\n",
        "\n",
        "    for i_step in range(n_steps):\n",
        "        z = z + h * f(z, t)\n",
        "        t = t + h\n",
        "    return z\n",
        "  \n",
        "  \n",
        "class ODEF(nn.Module):\n",
        "    def forward_with_grad(self, z, t, grad_outputs):\n",
        "        \"\"\"Compute f and a df/dz, a df/dp, a df/dt\"\"\"\n",
        "        batch_size = z.shape[0]\n",
        "\n",
        "        out = self.forward(z, t)\n",
        "\n",
        "        a = grad_outputs\n",
        "        adfdz, adfdt, *adfdp = torch.autograd.grad(\n",
        "            (out,), (z, t) + tuple(self.parameters()), grad_outputs=(a),\n",
        "            allow_unused=True, retain_graph=True\n",
        "        )\n",
        "        # grad method automatically sums gradients for batch items, we have to expand them back \n",
        "        if adfdp is not None:\n",
        "           adfdp = torch.cat([p_grad.flatten() for p_grad in adfdp]).unsqueeze(0)\n",
        "           adfdp = adfdp.expand(batch_size, -1) / batch_size\n",
        "        if adfdt is not None:\n",
        "           adfdt = adfdt.expand(batch_size, 1) / batch_size\n",
        "        return out, adfdz, adfdt, adfdp\n",
        "\n",
        "    def flatten_parameters(self):\n",
        "        p_shapes = []\n",
        "        flat_parameters = []\n",
        "        for p in self.parameters():\n",
        "            p_shapes.append(p.size())\n",
        "            flat_parameters.append(p.flatten())\n",
        "        return torch.cat(flat_parameters)      \n",
        "      \n",
        "class ODEAdjoint(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, z0, t, flat_parameters, func):\n",
        "        assert isinstance(func, ODEF)\n",
        "        bs, *z_shape = z0.size()\n",
        "        time_len = t.size(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            z = torch.zeros(time_len, bs, *z_shape).to(z0)\n",
        "            z[0] = z0\n",
        "            for i_t in range(time_len - 1):\n",
        "                z0 = ode_solve(z0, t[i_t], t[i_t+1], func)\n",
        "                z[i_t+1] = z0\n",
        "\n",
        "        ctx.func = func\n",
        "        ctx.save_for_backward(t, z.clone(), flat_parameters)\n",
        "        return z\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, dLdz):\n",
        "        \"\"\"\n",
        "        dLdz shape: time_len, batch_size, *z_shape\n",
        "        \"\"\"\n",
        "        func = ctx.func\n",
        "        t, z, flat_parameters = ctx.saved_tensors\n",
        "        time_len, bs, *z_shape = z.size()\n",
        "        n_dim = np.prod(z_shape)\n",
        "        n_params = flat_parameters.size(0)\n",
        "\n",
        "        # Dynamics of augmented system to be calculated backwards in time\n",
        "        def augmented_dynamics(aug_z_i, t_i):\n",
        "            \"\"\"\n",
        "            tensors here are temporal slices\n",
        "            t_i - is tensor with size: bs, 1\n",
        "            aug_z_i - is tensor with size: bs, n_dim*2 + n_params + 1\n",
        "            \"\"\"\n",
        "            z_i, a = aug_z_i[:, :n_dim], aug_z_i[:, n_dim:2*n_dim]  # ignore parameters and time\n",
        "\n",
        "            # Unflatten z and a\n",
        "            z_i = z_i.view(bs, *z_shape)\n",
        "            a = a.view(bs, *z_shape)\n",
        "            with torch.set_grad_enabled(True):\n",
        "                t_i = t_i.detach().requires_grad_(True)\n",
        "                z_i = z_i.detach().requires_grad_(True)\n",
        "                func_eval, adfdz, adfdt, adfdp = func.forward_with_grad(z_i, t_i, grad_outputs=a)  # bs, *z_shape\n",
        "                adfdz = adfdz.to(z_i) if adfdz is not None else torch.zeros(bs, *z_shape).to(z_i)\n",
        "                adfdp = adfdp.to(z_i) if adfdp is not None else torch.zeros(bs, n_params).to(z_i)\n",
        "                adfdt = adfdt.to(z_i) if adfdt is not None else torch.zeros(bs, 1).to(z_i)\n",
        "\n",
        "            # Flatten f and adfdz\n",
        "            func_eval = func_eval.view(bs, n_dim)\n",
        "            adfdz = adfdz.view(bs, n_dim) \n",
        "            return torch.cat((func_eval, -adfdz, -adfdp, -adfdt), dim=1)\n",
        "\n",
        "        dLdz = dLdz.view(time_len, bs, n_dim)  # flatten dLdz for convenience\n",
        "        with torch.no_grad():\n",
        "            ## Create placeholders for output gradients\n",
        "            # Prev computed backwards adjoints to be adjusted by direct gradients\n",
        "            adj_z = torch.zeros(bs, n_dim).to(dLdz)\n",
        "            adj_p = torch.zeros(bs, n_params).to(dLdz)\n",
        "            # In contrast to z and p we need to return gradients for all times\n",
        "            adj_t = torch.zeros(time_len, bs, 1).to(dLdz)\n",
        "\n",
        "            for i_t in range(time_len-1, 0, -1):\n",
        "                z_i = z[i_t]\n",
        "                t_i = t[i_t]\n",
        "                f_i = func(z_i, t_i).view(bs, n_dim)\n",
        "\n",
        "                # Compute direct gradients\n",
        "                dLdz_i = dLdz[i_t]\n",
        "                dLdt_i = torch.bmm(torch.transpose(dLdz_i.unsqueeze(-1), 1, 2), f_i.unsqueeze(-1))[:, 0]\n",
        "\n",
        "                # Adjusting adjoints with direct gradients\n",
        "                adj_z += dLdz_i\n",
        "                adj_t[i_t] = adj_t[i_t] - dLdt_i\n",
        "\n",
        "                # Pack augmented variable\n",
        "                aug_z = torch.cat((z_i.view(bs, n_dim), adj_z, torch.zeros(bs, n_params).to(z), adj_t[i_t]), dim=-1)\n",
        "\n",
        "                # Solve augmented system backwards\n",
        "                aug_ans = ode_solve(aug_z, t_i, t[i_t-1], augmented_dynamics)\n",
        "\n",
        "                # Unpack solved backwards augmented system\n",
        "                adj_z[:] = aug_ans[:, n_dim:2*n_dim]\n",
        "                adj_p[:] += aug_ans[:, 2*n_dim:2*n_dim + n_params]\n",
        "                adj_t[i_t-1] = aug_ans[:, 2*n_dim + n_params:]\n",
        "\n",
        "                del aug_z, aug_ans\n",
        "\n",
        "            ## Adjust 0 time adjoint with direct gradients\n",
        "            # Compute direct gradients \n",
        "            dLdz_0 = dLdz[0]\n",
        "            dLdt_0 = torch.bmm(torch.transpose(dLdz_0.unsqueeze(-1), 1, 2), f_i.unsqueeze(-1))[:, 0]\n",
        "\n",
        "            # Adjust adjoints\n",
        "            adj_z += dLdz_0\n",
        "            adj_t[0] = adj_t[0] - dLdt_0\n",
        "        return adj_z.view(bs, *z_shape), adj_t, adj_p, None\n",
        "\n",
        " \n",
        "class NeuralODE(nn.Module):\n",
        "    def __init__(self, func):\n",
        "        super(NeuralODE, self).__init__()\n",
        "        assert isinstance(func, ODEF)\n",
        "        self.func = func\n",
        "\n",
        "    def forward(self, z0, t=Tensor([0., 1.]), return_whole_sequence=False):\n",
        "        t = t.to(z0)\n",
        "        z = ODEAdjoint.apply(z0, t, self.func.flatten_parameters(), self.func)\n",
        "        if return_whole_sequence:\n",
        "            return z\n",
        "        else:\n",
        "            return z[-1]\n",
        "\n",
        "  \n",
        "def add_time(in_tensor, t):\n",
        "    bs, c, w, h = in_tensor.shape\n",
        "    return torch.cat((in_tensor, t.expand(bs, 1, w, h)), dim=1)\n",
        "  \n",
        "  \n",
        "  \n",
        "class ConvODEF(ODEF):\n",
        "    def __init__(self, dim):\n",
        "        super(ConvODEF, self).__init__()\n",
        "\n",
        "        self.kernel_size=3\n",
        "          \n",
        " \n",
        "        self.conv2d=torch.nn.Conv2d(kernel_size=3, in_channels=dim+1, out_channels=dim,padding=1)\n",
        "        self.bn=torch.nn.BatchNorm2d(dim)\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        xt = add_time(x, t)\n",
        "        #print('shape=',xt.shape)\n",
        "        h = self.bn(torch.relu(self.conv2d(xt)))\n",
        "        ht = add_time(h, t)\n",
        "        dxdt = self.bn(torch.relu(self.conv2d(ht)))\n",
        "        return dxdt\n",
        "      \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY9a1GB14sTG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "func = ConvODEF(64)\n",
        "#print(func)\n",
        "ode = NeuralODE(func)\n",
        "#print(ode)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAA6P9T44nTp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SEM_SEG_ODE(nn.Module):\n",
        "    def __init__(self, ode,n_class,input_shape):\n",
        "        super(SEM_SEG_ODE, self).__init__()\n",
        "        self.feature = ode\n",
        "        self.input_shape=input_shape[1]\n",
        "        self.n_filters=16\n",
        "        self.dropout=.05\n",
        "        self.batchnorm=True\n",
        "        self.kernel_size=3\n",
        "        self.n_class=n_class\n",
        "          \n",
        "        self.relu=torch.nn.ReLU(inplace=True)\n",
        "        self.maxpool=torch.nn.MaxPool2d(2)\n",
        "        self.dropout=torch.nn.Dropout2d(.05)\n",
        "        self.conv2d=torch.nn.Conv2d(kernel_size=3, in_channels=input_shape[2], out_channels=16,padding=1)\n",
        "        self.bn=torch.nn.BatchNorm2d(16)\n",
        "\n",
        "        self.conv2d_1=torch.nn.Conv2d(kernel_size=3, in_channels=16, out_channels=32,padding=1)\n",
        "        self.bn1=torch.nn.BatchNorm2d(32)\n",
        "        \n",
        "        self.conv2d_2=torch.nn.Conv2d(kernel_size=3, in_channels=32, out_channels=64,padding=1)\n",
        "        self.bn2=torch.nn.BatchNorm2d(64)\n",
        "        \n",
        "        self.conv2d_3=torch.nn.Conv2d(kernel_size=3, in_channels=64, out_channels=128,padding=1)\n",
        "        self.bn3=torch.nn.BatchNorm2d(128)\n",
        "\n",
        "        self.conv2d_4=torch.nn.Conv2d(kernel_size=3, in_channels=128, out_channels=256,padding=1)\n",
        "        self.bn4=torch.nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv2d_5=torch.nn.Conv2d(kernel_size=3, in_channels=256, out_channels=128,padding=1)\n",
        "        self.bn5=torch.nn.BatchNorm2d(128)\n",
        "\n",
        "        self.conv2d_6=torch.nn.Conv2d(kernel_size=3, in_channels=128, out_channels=64,padding=1)\n",
        "        self.bn6=torch.nn.BatchNorm2d(64)\n",
        "\n",
        "        self.conv2d_7=torch.nn.Conv2d(kernel_size=3, in_channels=64, out_channels=32,padding=1)\n",
        "        self.bn7=torch.nn.BatchNorm2d(32)\n",
        "\n",
        "        self.conv2d_8=torch.nn.Conv2d(kernel_size=3, in_channels=32, out_channels=16,padding=1)\n",
        "        self.bn8=torch.nn.BatchNorm2d(16)\n",
        "\n",
        "        self.up=torch.nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.up1=torch.nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.up2=torch.nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.up3=torch.nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.conv2d_9=torch.nn.Conv2d(kernel_size=1, in_channels=16, out_channels=self.n_class,padding=0)\n",
        "        self.sig=torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "\n",
        "        c1=self.conv2d(x)\n",
        "        if self.batchnorm:\n",
        "          c1=self.bn(c1)\n",
        "        c1=self.relu(c1)\n",
        "\n",
        "        p1=self.maxpool(c1)\n",
        "        p1=self.dropout(p1)\n",
        "\n",
        "        c2=self.conv2d_1(p1)\n",
        "        if self.batchnorm:\n",
        "          c2=self.bn1(c2)\n",
        "        c2=self.relu(c2)\n",
        "\n",
        "        p2=self.maxpool(c2)\n",
        "        p2=self.dropout(p2)\n",
        "\n",
        "        c3=self.conv2d_2(p2)\n",
        "        if self.batchnorm:\n",
        "          c3=self.bn2(c3)\n",
        "        c3=self.relu(c3)\n",
        "\n",
        "        p3=self.maxpool(c3)\n",
        "        p3=self.dropout(p3)\n",
        "\n",
        "        ode_here=self.feature(p3)#64-64\n",
        "\n",
        "\n",
        "        c4=self.conv2d_3(ode_here)#64-128\n",
        "        if self.batchnorm:\n",
        "          c4=self.bn3(c4)\n",
        "        c4=self.relu(c4)\n",
        "\n",
        "        p4=self.maxpool(c4)\n",
        "        p4=self.dropout(p4)\n",
        "        \n",
        "        c5=self.conv2d_4(p4)#128-256\n",
        "        if self.batchnorm:\n",
        "          c5=self.bn4(c5)\n",
        "        c5=self.relu(c5)\n",
        "        \n",
        "        u6=self.up(c5)#256-128\n",
        "        u6=torch.cat((u6,c4),1)\n",
        "        u6=self.dropout(u6)\n",
        "\n",
        "        c6=self.conv2d_5(u6)\n",
        "        if self.batchnorm:\n",
        "          c6=self.bn5(c6)\n",
        "        c6=self.relu(c6)\n",
        "\n",
        "        u7=self.up1(c6)#128-64\n",
        "        u7=torch.cat((u7,c3),1)\n",
        "        u7=self.dropout(u7)\n",
        "        \n",
        "\n",
        "        c7=self.conv2d_6(u7)#64-32\n",
        "        if self.batchnorm:\n",
        "          c7=self.bn6(c7)\n",
        "        c7=self.relu(c7)\n",
        "\n",
        "        u8=self.up2(c7)\n",
        "        u8=torch.cat((u8,c2),1)\n",
        "        u8=self.dropout(u8)\n",
        "        \n",
        "\n",
        "        c8=self.conv2d_7(u8)\n",
        "        if self.batchnorm:\n",
        "          c8=self.bn7(c8)\n",
        "        c8=self.relu(c8)\n",
        "\n",
        "        u9=self.up3(c8)\n",
        "        u9=torch.cat((u9,c1),1)\n",
        "        u9=self.dropout(u9)\n",
        "        \n",
        "\n",
        "        c9=self.conv2d_8(u9)\n",
        "        if self.batchnorm:\n",
        "          c9=self.bn8(c9)\n",
        "        c9=self.relu(c9)\n",
        "\n",
        "        output=self.conv2d_9(c9)\n",
        "        #output=self.sig(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_x1Jus1ohyHI",
        "colab_type": "code",
        "outputId": "134f8f2b-90c5-4e90-ea66-9800e71e8306",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = SEM_SEG_ODE(ode,n_class=6,input_shape=(192,192,3))\n",
        "if use_cuda:\n",
        "    model = model.cuda()\n",
        "\n",
        "summary(model, (3, 192, 192))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7biEw9y7ImiO",
        "colab_type": "text"
      },
      "source": [
        "# defining metrics function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXOUMLY1wKUL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "import torch.nn.functional as F\n",
        "from loss import dice_loss\n",
        "loss1=nn.CrossEntropyLoss()\n",
        "sig=nn.Sigmoid()\n",
        "\n",
        "def calc_metrics(pred, target, metrics, bce_weight=0.5):\n",
        "    bce = F.binary_cross_entropy_with_logits(pred, target)\n",
        "\n",
        "    pred = F.sigmoid(pred)\n",
        "    dice = dice_loss(pred, target)\n",
        "\n",
        "    loss = bce * bce_weight + dice * (1 - bce_weight)\n",
        "\n",
        "    numerator =  (pred * target).sum((1, 2, 3))\n",
        "    denominator = (pred + target).sum((1, 2, 3))\n",
        "\n",
        "    iou=(numerator / (denominator-numerator)).mean()\n",
        "\n",
        "\n",
        "    # pred1=pred.data.cpu().numpy()\n",
        "    # target1=target.data.cpu().numpy()\n",
        "    # pred1=np.argmax(pred1,axis=1)\n",
        "    # target1=np.argmax(target1,axis=1)\n",
        "    # #print(pred1.shape)\n",
        "    # pred1=np.reshape(pred1,(pred1.shape[0]*pred1.shape[1]*pred1.shape[2]))\n",
        "    # target1=np.reshape(target1,(target1.shape[0]*target1.shape[1]*target1.shape[2]))\n",
        "    # same=0\n",
        "    # #print(pred.shape)\n",
        "    # for i in range(0,pred1.shape[0]):\n",
        "    #   if pred1[i]==target1[i]:\n",
        "    #     same=same+1\n",
        "    # acc=same/pred1.shape[0]\n",
        "    #print(acc)\n",
        "\n",
        "    # _, preds = torch.max(pred, dim=1)\n",
        "    # _, targets = torch.max(target, dim=1)\n",
        "    # valid = (targets >= 0).long()\n",
        "    # acc_sum = torch.sum(valid * (preds == targets).long())\n",
        "    # pixel_sum = torch.sum(valid)\n",
        "    # acc = acc_sum.float() / (pixel_sum.float() + 1e-10)\n",
        "\n",
        "\n",
        "    #acc=(torch.sum((torch.argmax(pred,dim=1)==torch.argmax(target,dim=1))).item())/(target.shape[0]*target.shape[1]*target.shape[2]*target.shape[3])\n",
        "    #print(acc)\n",
        "\n",
        "    #print(target.size(0))\n",
        "    metrics['bce'] += bce.data.cpu().numpy() * target.size(0)\n",
        "    metrics['dice'] += dice.data.cpu().numpy() * target.size(0)\n",
        "    metrics['loss'] += loss.data.cpu().numpy() * target.size(0)\n",
        "    metrics['iou'] += iou.data.cpu().numpy() * target.size(0)\n",
        "   # metrics['acc'] +=acc* target.size(0)\n",
        "\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_WTf6g4wL7M",
        "colab_type": "text"
      },
      "source": [
        "# Defining training functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYDhK1iBJk8C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def print_metrics(metrics, epoch_samples, phase):\n",
        "    outputs = []\n",
        "    for k in metrics.keys():\n",
        "        outputs.append(\"{}: {:4f}\".format(k, metrics[k] / epoch_samples))\n",
        "\n",
        "    print(\"{}: {}\".format(phase, \", \".join(outputs)))\n",
        "\n",
        "def train_model(model, optimizer, scheduler, num_epochs=25):\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = 1e10\n",
        "    best_iou=-1\n",
        "    best_dice=1e10\n",
        "    best_bce=1e10\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        since = time.time()\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "                for param_group in optimizer.param_groups:\n",
        "                    print(\"LR\", param_group['lr'])\n",
        "\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            metrics = defaultdict(float)\n",
        "            epoch_samples = 0\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                #print(inputs.shape,labels.shape)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    #print(outputs.shape)\n",
        "                    loss = calc_metrics(outputs, labels, metrics)\n",
        "                    #print(loss.item())\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                epoch_samples += inputs.size(0)\n",
        "\n",
        "            print_metrics(metrics, epoch_samples, phase)\n",
        "            epoch_loss = metrics['loss'] / epoch_samples\n",
        "            epoch_dice = metrics['dice'] / epoch_samples\n",
        "            epoch_bce = metrics['bce'] / epoch_samples\n",
        "            epoch_iou = metrics['iou'] / epoch_samples\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val':\n",
        "              if epoch_loss < best_loss:\n",
        "                print(\"saving best model\")\n",
        "                best_loss = epoch_loss\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "              if epoch_dice<best_dice:\n",
        "                best_dice=epoch_dice\n",
        "              if epoch_bce<best_bce:\n",
        "                best_bce=epoch_bce\n",
        "              if epoch_iou>best_iou:\n",
        "                best_iou=epoch_iou\n",
        "\n",
        "        time_elapsed = time.time() - since\n",
        "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "\n",
        "    print('Best val loss: {:4f}'.format(best_loss))\n",
        "    print('Best val dice: {:4f}'.format(best_dice))\n",
        "    print('Best val bce: {:4f}'.format(best_bce))\n",
        "    print('Best val iou: {:4f}'.format(best_iou))\n",
        "     # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkOvby9yIzIV",
        "colab_type": "text"
      },
      "source": [
        "# training the unet ode model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoN1KGFyqdBm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "94e8b034-f58b-4496-b18d-023a99926c8c"
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import time\n",
        "import copy\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "optimizer_ft = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
        "\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=200, gamma=0.1)\n",
        "\n",
        "model = train_model(model, optimizer_ft, exp_lr_scheduler, num_epochs=500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ohdZbaIIuiE",
        "colab_type": "text"
      },
      "source": [
        "# making prediction on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhHyDuShSqm0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "def pred_small_dataset(my_model):\n",
        "\n",
        "    my_model.eval()   # Set model to the evaluation mode\n",
        "\n",
        "    # Create another simulation dataset for test\n",
        "    test_dataset = SimDataset(3, transform = trans)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=3, shuffle=False, num_workers=0)\n",
        "\n",
        "    # Get the first batch\n",
        "    inputs, labels = next(iter(test_loader))\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # Predict\n",
        "    pred = model(inputs)\n",
        "    # The loss functions include the sigmoid function.\n",
        "    pred = F.sigmoid(pred)\n",
        "    #print(torch.sum(pred,dim=1))\n",
        "    pred = pred.data.cpu().numpy()\n",
        "    #print(pred.shape)\n",
        "\n",
        "    # Change channel-order and make 3 channels for matplot\n",
        "    input_images_rgb = [reverse_transform(x) for x in inputs.cpu()]\n",
        "\n",
        "    # Map each channel (i.e. class) to each color\n",
        "    target_masks_rgb = [helper.masks_to_colorimg(x) for x in labels.cpu().numpy()]\n",
        "    pred_rgb = [helper.masks_to_colorimg(x) for x in pred]\n",
        "\n",
        "    helper.plot_side_by_side([input_images_rgb, target_masks_rgb, pred_rgb])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtN3CstN_-Ll",
        "colab_type": "code",
        "outputId": "fe526040-ead9-4a13-a834-76a41a77db8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758
        }
      },
      "source": [
        "pred_small_dataset(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryova0hu0FgL",
        "colab_type": "text"
      },
      "source": [
        "# downloading data from kaggle neuclie segmentation dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk5z1ZUh0MMQ",
        "colab_type": "code",
        "outputId": "9c264b4e-0617-431d-e98a-7419fc1d56a8",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 448
        }
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "!kaggle competitions download -c data-science-bowl-2018\n",
        "\n",
        "!unzip -q '/content/stage1_sample_submission.csv.zip' -d '/content/kaggle_neuclei'\n",
        "!unzip -q '/content/stage1_solution.csv.zip' -d '/content/kaggle_neuclei'\n",
        "!unzip -q '/content/stage1_test.zip' -d '/content/kaggle_neuclei/stage1_test'\n",
        "!unzip -q '/content/stage1_train.zip' -d '/content/kaggle_neuclei/stage1_train'\n",
        "!unzip -q '/content/stage1_train_labels.csv.zip' -d '/content/kaggle_neuclei'\n",
        "!unzip -q '/content/stage2_sample_submission_final.csv.zip' -d '/content/kaggle_neuclei'\n",
        "!unzip -q '/content/stage2_test_final.zip' -d '/content/kaggle_neuclei/stage2_test_final'\n",
        "\n",
        "!rm '/content/stage1_sample_submission.csv.zip'\n",
        "!rm '/content/stage1_solution.csv.zip'\n",
        "!rm '/content/stage1_test.zip'\n",
        "!rm '/content/stage1_train.zip'\n",
        "!rm '/content/stage1_train_labels.csv.zip'\n",
        "!rm '/content/stage2_sample_submission_final.csv.zip'\n",
        "!rm '/content/stage2_test_final.zip'\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sPaDuhPPnoC",
        "colab_type": "text"
      },
      "source": [
        "# Dataloader for neuclie segmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bahTAKuRKJz",
        "colab_type": "code",
        "outputId": "4e6de7d3-0e21-4d61-dbef-b57edb0265fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_ids=os.listdir('/content/kaggle_neuclei/stage1_train')\n",
        "#train_ids=train_ids[:int(len(train_ids)*.20)]\n",
        "train_ids,val_ids=train_test_split(train_ids,test_size=.1,random_state=42)\n",
        "print(len(train_ids))\n",
        "print(len(val_ids))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TceJf5z3R5U1",
        "colab_type": "code",
        "outputId": "61684bad-cb24-4478-b5d9-e8b8b40cc336",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "train_img=cv2.imread('/content/kaggle_neuclei/stage1_train/00071198d059ba7f5914a526d124d28e6d010c92466da21d4a04cd5413362552/images/00071198d059ba7f5914a526d124d28e6d010c92466da21d4a04cd5413362552.png')\n",
        "plt.imshow(train_img)\n",
        "plt.show()\n",
        "plt.figure()\n",
        "\n",
        "mask_img=np.zeros(shape=(train_img.shape[0],train_img.shape[1],3))\n",
        "\n",
        "mask_one_file=os.listdir('/content/kaggle_neuclei/stage1_train/00071198d059ba7f5914a526d124d28e6d010c92466da21d4a04cd5413362552/masks')\n",
        "for m in mask_one_file:\n",
        "  single_mask=cv2.imread('/content/kaggle_neuclei/stage1_train/00071198d059ba7f5914a526d124d28e6d010c92466da21d4a04cd5413362552/masks/'+m)\n",
        "  mask_img=np.maximum(mask_img,single_mask)\n",
        "  # for i in range(0,train_img.shape[0]):\n",
        "  #   for j in range(0,train_img.shape[1]):\n",
        "  #     mask_img[i,j]=single_mask[i,j][0]\n",
        "\n",
        "plt.imshow(mask_img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMSrqEEB1ur9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Rescale:\n",
        "    def __init__(self, output_size: int):\n",
        "        assert isinstance(output_size, (int, tuple))\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def __call__(self, image: np.ndarray) -> np.ndarray:\n",
        "        return cv2.resize(image, (self.output_size, self.output_size), cv2.INTER_AREA)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThG-41oePs54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeucleiDataLoader(Dataset):\n",
        "    def __init__(self, img_ids,path_pref,im_shape, transform=None,mask_transform=None):\n",
        "        self.transform = transform\n",
        "        self.img_ids=img_ids\n",
        "        self.path_pref=path_pref#'/content/kaggle_neuclei/stage1_train/'\n",
        "        self.im_shape=im_shape\n",
        "        self.mask_transform=mask_transform\n",
        "\n",
        "    def __len__(self):      \n",
        "        return len(self.img_ids)\n",
        "\n",
        "    def __getitem__(self, idx):      \n",
        "        image = cv2.imread(self.path_pref+self.img_ids[idx]+'/images/'+self.img_ids[idx]+'.png')\n",
        "        target_img = np.zeros(image.shape[:2], dtype=np.uint8)\n",
        "        mask_one_file=os.listdir(self.path_pref+self.img_ids[idx]+'/masks')\n",
        "        for m in mask_one_file:\n",
        "            single_mask=cv2.imread(self.path_pref+self.img_ids[idx]+'/masks/'+m,0)\n",
        "            #print(single_mask.shape)\n",
        "            target_img=np.maximum(target_img,single_mask)\n",
        "        \n",
        "        if self.transform:\n",
        "          image=self.transform(image)\n",
        "        if self.mask_transform:\n",
        "          target_img=self.mask_transform(target_img)\n",
        "\n",
        "        return [image, target_img]\n",
        "\n",
        "\n",
        "trans = transforms.Compose([\n",
        "    Rescale(256),\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # imagenet\n",
        "])\n",
        "\n",
        "\n",
        "train_set = NeucleiDataLoader(train_ids,'/content/kaggle_neuclei/stage1_train/',256,trans,trans)\n",
        "val_set = NeucleiDataLoader(val_ids,'/content/kaggle_neuclei/stage1_train/',256,trans,trans)\n",
        "\n",
        "image_datasets = {\n",
        "    'train': train_set, 'val': val_set\n",
        "}\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "neuclie_dataloaders = {\n",
        "    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n",
        "    'val': DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ_0boXQvo-T",
        "colab_type": "text"
      },
      "source": [
        "**Testing the dataloader**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SwxSKDnsSwW",
        "colab_type": "code",
        "outputId": "6086d8f5-fefa-41c2-f586-5296407b22c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        }
      },
      "source": [
        "image,mask=next(iter(neuclie_dataloaders['val']))\n",
        "print(image.shape,mask.shape)\n",
        "from torchvision.transforms import transforms, ToTensor, ToPILImage\n",
        "def reverse_transform(inp,mask):   \n",
        "    output_transforms = transforms.Compose([\n",
        "            ToPILImage()])\n",
        "    mask=output_transforms(mask) \n",
        "    inp=output_transforms(inp)\n",
        "   \n",
        "    return inp,mask\n",
        "\n",
        "image,mask=reverse_transform(image[0],mask[0])\n",
        "plt.imshow(image)\n",
        "plt.title('input image')\n",
        "plt.figure()\n",
        "plt.imshow(mask,cmap='gray')\n",
        "plt.title('target mask')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFU4dwPi4JhL",
        "colab_type": "text"
      },
      "source": [
        "# Defining the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ihknofv4NPM",
        "colab_type": "code",
        "outputId": "9970b7e5-bd8c-442e-e6d3-0d20cdbe6311",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = SEM_SEG_ODE(ode,n_class=1,input_shape=(256,256,3))\n",
        "if use_cuda:\n",
        "    model = model.cuda()\n",
        "\n",
        "summary(model, (3, 256, 256))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRggSFZp7Zv9",
        "colab_type": "text"
      },
      "source": [
        "# defining the metrics function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAUoXqTf7dup",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "import torch.nn.functional as F\n",
        "from loss import dice_loss\n",
        "ce=nn.CrossEntropyLoss()\n",
        "def dice_loss1(input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Soft dice loss = 2*|A∩B| / |A|+|B|\n",
        "    \"\"\"\n",
        "    numerator = 2 * (input * target).sum((1, 2, 3))\n",
        "    denominator = (input + target).sum((1, 2, 3))\n",
        "\n",
        "    return (1 - numerator / denominator).mean()\n",
        "\n",
        "\n",
        "    \n",
        "def calc_metrics(pred, target, metrics, bce_weight=0.5):\n",
        "    bce = F.binary_cross_entropy_with_logits(pred, target)\n",
        "\n",
        "    pred = F.sigmoid(pred)\n",
        "    dice = dice_loss1(pred, target)\n",
        "\n",
        "    loss = bce  + dice #* (1 - bce_weight)\n",
        "\n",
        "    numerator =  (pred * target).sum((1, 2, 3))\n",
        "    denominator = (pred + target).sum((1, 2, 3))\n",
        "\n",
        "    iou=(numerator / (denominator-numerator)).mean()\n",
        "\n",
        "    # pred1=pred.data.cpu().numpy()\n",
        "    # target1=target.data.cpu().numpy()\n",
        "    # #pred1=np.argmax(pred1,axis=1)\n",
        "    # #target1=np.argmax(target1,axis=1)\n",
        "    # #print(pred1.shape)\n",
        "    # pred1=np.reshape(pred1,(pred1.shape[0]*pred1.shape[1]*pred1.shape[2]*pred1.shape[3]))\n",
        "    # target1=np.reshape(target1,(target1.shape[0]*target1.shape[1]*target1.shape[2]*target1.shape[3]))\n",
        "    # same=0\n",
        "    #print(pred.shape)\n",
        "    # for i in range(0,pred1.shape[0]):\n",
        "    #   if pred1[i]>=.5:\n",
        "    #     pred1[i]=1.0\n",
        "    #   if pred1[i]<.5:\n",
        "    #     pred1[i]=0.0\n",
        "    #   if pred1[i]==target1[i]:\n",
        "    #     same=same+1\n",
        "    # acc=same/pred1.shape[0]\n",
        "    # #print(acc)\n",
        "\n",
        "    metrics['bce'] += bce.data.cpu().numpy() * target.size(0)\n",
        "    metrics['dice'] += dice.data.cpu().numpy() * target.size(0)\n",
        "    metrics['loss'] += loss.data.cpu().numpy() * target.size(0)\n",
        "    metrics['iou'] +=iou.data.cpu().numpy() * target.size(0)\n",
        "    #metrics['acc'] +=acc * target.size(0)\n",
        "\n",
        "\n",
        "    return loss\n",
        "\n",
        "# def calc_ce_loss(pred,target,metrics):\n",
        "#   target=target.data.cpu().numpy()\n",
        "#   target=np.argmax(target,axis=1)\n",
        "#   #print(target.shape)\n",
        "#   target=torch.from_numpy(target)\n",
        "#   target=target.cuda()\n",
        "#   ce_loss=ce(pred,target)\n",
        "#   #print(ce_loss.data)\n",
        "#   metrics['ce']+=ce_loss.data.cpu().numpy()*target.size(0)\n",
        "#   return ce_loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDdUvGxs79LZ",
        "colab_type": "text"
      },
      "source": [
        "# Defining training function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8mzR1rM78mi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def print_metrics(metrics, epoch_samples, phase):\n",
        "    outputs = []\n",
        "    for k in metrics.keys():\n",
        "        outputs.append(\"{}: {:4f}\".format(k, metrics[k] / epoch_samples))\n",
        "\n",
        "    print(\"{}: {}\".format(phase, \", \".join(outputs)))\n",
        "\n",
        "def train_model(model, optimizer, scheduler, num_epochs=25):\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = 1e10\n",
        "    best_iou=-1\n",
        "    best_dice=1e10\n",
        "    best_bce=1e10\n",
        "    #best_dice_co=-1\n",
        "\n",
        "\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for_once=1\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        since = time.time()\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "                for param_group in optimizer.param_groups:\n",
        "                    print(\"LR\", param_group['lr'])\n",
        "\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            metrics = defaultdict(float)\n",
        "            epoch_samples = 0\n",
        "\n",
        "            for inputs, labels in neuclie_dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                #inputs=inputs.type(torch.cuda.FloatTensor)\n",
        "                #labels=labels.type(torch.cuda.FloatTensor)\n",
        "                #print(inputs.shape,labels.shape)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    # if for_once==1:\n",
        "\n",
        "                    #   img,segmented_mask=reverse_transform(inputs[0],outputs[0])\n",
        "                    #   img,mask=reverse_transform(inputs[0],labels[0])\n",
        "                    #   plt.imshow(img)\n",
        "                    #   plt.title('input image')\n",
        "                    #   plt.figure()\n",
        "                    #   plt.imshow(mask,cmap='gray')\n",
        "                    #   plt.title('target mask')\n",
        "\n",
        "                    #   plt.figure()\n",
        "                    #   plt.imshow(segmented_mask,cmap='gray')\n",
        "                    #   plt.title('predected mask')\n",
        "                    #   for_once=0\n",
        "\n",
        "                    #print(outputs.shape)\n",
        "                    #outputs=outputs.int()\n",
        "                    loss = calc_metrics(outputs, labels, metrics)\n",
        "\n",
        "                    #print(loss.item())\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                epoch_samples += inputs.size(0)\n",
        "\n",
        "            print_metrics(metrics, epoch_samples, phase)\n",
        "            epoch_loss = metrics['loss'] / epoch_samples\n",
        "            epoch_dice = metrics['dice'] / epoch_samples\n",
        "            epoch_bce = metrics['bce'] / epoch_samples\n",
        "            epoch_iou = metrics['iou'] / epoch_samples\n",
        "            #epoch_dice_co=metrics['dice_co'] / epoch_samples\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val':\n",
        "              if epoch_loss < best_loss:\n",
        "                print(\"saving best model\")\n",
        "                best_loss = epoch_loss\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "              if epoch_dice<best_dice:\n",
        "                best_dice=epoch_dice\n",
        "              if epoch_bce<best_bce:\n",
        "                best_bce=epoch_bce\n",
        "              if epoch_iou>best_iou:\n",
        "                best_iou=epoch_iou\n",
        "              # if epoch_dice_co>best_dice_co:\n",
        "              #   best_dice_co=epoch_dice_co\n",
        "\n",
        "        time_elapsed = time.time() - since\n",
        "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "        #for_once=0\n",
        "\n",
        "    print('Best val loss: {:4f}'.format(best_loss))\n",
        "    print('Best val dice: {:4f}'.format(best_dice))\n",
        "    print('Best val bce: {:4f}'.format(best_bce))\n",
        "    print('Best val iou: {:4f}'.format(best_iou))\n",
        "    #print('Best val dice_co: {:4f}'.format(best_dice_co))\n",
        "     # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WdzFxmj8PzX",
        "colab_type": "text"
      },
      "source": [
        "# Training the odeunet model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2ZzUn0I8SdM",
        "colab_type": "code",
        "outputId": "e3463e41-60e5-467e-a63a-bbc9f9578993",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import time\n",
        "import copy\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "optimizer_ft = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
        "\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=20, gamma=0.1)\n",
        "\n",
        "model = train_model(model, optimizer_ft, exp_lr_scheduler, num_epochs=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvqzK5LqFQsD",
        "colab_type": "text"
      },
      "source": [
        "# Training the fcn/unet model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQ47CqYbFUqj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "137bebda-f4c7-40a8-edd2-1d7c480e6f3f"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "fcn_unet_nuclei = ResNetUNet(n_class=1)\n",
        "fcn_unet_nuclei = fcn_unet_nuclei.to(device)\n",
        "\n",
        "# check keras-like model summary using torchsummary\n",
        "from torchsummary import summary\n",
        "print(summary(fcn_unet_nuclei, input_size=(3, 192, 192)))\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import time\n",
        "import copy\n",
        "\n",
        "optimizer_ft = torch.optim.Adam(filter(lambda p: p.requires_grad, fcn_unet_nuclei.parameters()), lr=1e-3)\n",
        "\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=20, gamma=0.1)\n",
        "\n",
        "fcn_unet_nuclei = train_model(fcn_unet_nuclei, optimizer_ft, exp_lr_scheduler, num_epochs=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Rsr-rQvHEaf",
        "colab_type": "text"
      },
      "source": [
        "# Training Unet Buda"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHV35tZvHJS6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d8fab600-bb56-4d9c-f4b1-2a0db5a7ad66"
      },
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels=3, out_channels=1, init_features=32):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        features = init_features\n",
        "        self.encoder1 = UNet._block(in_channels, features, name=\"enc1\")\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder2 = UNet._block(features, features * 2, name=\"enc2\")\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder3 = UNet._block(features * 2, features * 4, name=\"enc3\")\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder4 = UNet._block(features * 4, features * 8, name=\"enc4\")\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.bottleneck = UNet._block(features * 8, features * 16, name=\"bottleneck\")\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose2d(\n",
        "            features * 16, features * 8, kernel_size=2, stride=2\n",
        "        )\n",
        "        self.decoder4 = UNet._block((features * 8) * 2, features * 8, name=\"dec4\")\n",
        "        self.upconv3 = nn.ConvTranspose2d(\n",
        "            features * 8, features * 4, kernel_size=2, stride=2\n",
        "        )\n",
        "        self.decoder3 = UNet._block((features * 4) * 2, features * 4, name=\"dec3\")\n",
        "        self.upconv2 = nn.ConvTranspose2d(\n",
        "            features * 4, features * 2, kernel_size=2, stride=2\n",
        "        )\n",
        "        self.decoder2 = UNet._block((features * 2) * 2, features * 2, name=\"dec2\")\n",
        "        self.upconv1 = nn.ConvTranspose2d(\n",
        "            features * 2, features, kernel_size=2, stride=2\n",
        "        )\n",
        "        self.decoder1 = UNet._block(features * 2, features, name=\"dec1\")\n",
        "\n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels=features, out_channels=out_channels, kernel_size=1\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc1 = self.encoder1(x)\n",
        "        enc2 = self.encoder2(self.pool1(enc1))\n",
        "        enc3 = self.encoder3(self.pool2(enc2))\n",
        "        enc4 = self.encoder4(self.pool3(enc3))\n",
        "\n",
        "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
        "\n",
        "        dec4 = self.upconv4(bottleneck)\n",
        "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
        "        dec4 = self.decoder4(dec4)\n",
        "        dec3 = self.upconv3(dec4)\n",
        "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
        "        dec3 = self.decoder3(dec3)\n",
        "        dec2 = self.upconv2(dec3)\n",
        "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
        "        dec2 = self.decoder2(dec2)\n",
        "        dec1 = self.upconv1(dec2)\n",
        "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
        "        dec1 = self.decoder1(dec1)\n",
        "        return self.conv(dec1)\n",
        "\n",
        "    @staticmethod\n",
        "    def _block(in_channels, features, name):\n",
        "        return nn.Sequential(\n",
        "            OrderedDict(\n",
        "                [\n",
        "                    (\n",
        "                        name + \"conv1\",\n",
        "                        nn.Conv2d(\n",
        "                            in_channels=in_channels,\n",
        "                            out_channels=features,\n",
        "                            kernel_size=3,\n",
        "                            padding=1,\n",
        "                            bias=False,\n",
        "                        ),\n",
        "                    ),\n",
        "                    (name + \"norm1\", nn.BatchNorm2d(num_features=features)),\n",
        "                    (name + \"relu1\", nn.ReLU(inplace=True)),\n",
        "                    (\n",
        "                        name + \"conv2\",\n",
        "                        nn.Conv2d(\n",
        "                            in_channels=features,\n",
        "                            out_channels=features,\n",
        "                            kernel_size=3,\n",
        "                            padding=1,\n",
        "                            bias=False,\n",
        "                        ),\n",
        "                    ),\n",
        "                    (name + \"norm2\", nn.BatchNorm2d(num_features=features)),\n",
        "                    (name + \"relu2\", nn.ReLU(inplace=True)),\n",
        "                ]\n",
        "            )\n",
        "        )\n",
        "buda_nuclei_model=UNet()\n",
        "\n",
        "if use_cuda:\n",
        "    buda_nuclei_model = buda_nuclei_model.cuda()\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "optimizer_ft = torch.optim.Adam(filter(lambda p: p.requires_grad, buda_nuclei_model.parameters()), lr=1e-3)\n",
        "\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=20, gamma=0.1)\n",
        "print(summary(buda_nuclei_model, (3, 256, 256)))\n",
        "buda_nuclei_model = train_model(buda_nuclei_model, optimizer_ft, exp_lr_scheduler, num_epochs=50)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9ZKJZXZZEi-",
        "colab_type": "text"
      },
      "source": [
        "# Predicting on some data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZ3NbOTMK8QE",
        "colab_type": "code",
        "outputId": "75e6950c-7a7a-4c1f-ef7a-21b86806d113",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 914
        }
      },
      "source": [
        "image,mask=next(iter(neuclie_dataloaders['val']))\n",
        "print(mask.shape)\n",
        "image=image.to(device)\n",
        "mask=mask.to(device)\n",
        "model=model.eval()\n",
        "outputs=model(image)\n",
        "outputs=F.sigmoid(outputs)\n",
        "print(outputs.shape)\n",
        "\n",
        "def reverse_transform(inp,mask):   \n",
        "    output_transforms = transforms.Compose([\n",
        "            ToPILImage()])\n",
        "    mask=mask.cpu()\n",
        "    inp=inp.cpu()\n",
        "    mask=output_transforms(mask) \n",
        "    inp=output_transforms(inp)\n",
        "   \n",
        "    return inp,mask\n",
        "\n",
        "img,segmented_mask=reverse_transform(image[0],outputs[0])\n",
        "img,mask=reverse_transform(image[0],mask[0])\n",
        "plt.imshow(img)\n",
        "plt.title('input image')\n",
        "plt.figure()\n",
        "plt.imshow(mask,cmap='gray')\n",
        "plt.title('target mask')\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(segmented_mask,cmap='gray')\n",
        "plt.title('predected mask')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUld5no8ZKps",
        "colab_type": "text"
      },
      "source": [
        "# predicting on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pEc8VC8XOF5",
        "colab_type": "code",
        "outputId": "9a8b5a87-fc98-4b5a-a709-b9f624af65ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        }
      },
      "source": [
        "model=model.eval()\n",
        "test_single=cv2.imread('/content/kaggle_neuclei/stage1_test/0114f484a16c152baa2d82fdd43740880a762c93f436c8988ac461c5c9dbe7d5/images/0114f484a16c152baa2d82fdd43740880a762c93f436c8988ac461c5c9dbe7d5.png')\n",
        "trans_test=transforms.Compose([\n",
        "            Rescale(256),\n",
        "            ToTensor()])\n",
        "test_single=trans_test(test_single)\n",
        "\n",
        "output = model(test_single.unsqueeze(0).cuda())  # Add batch dim\n",
        "output_transforms = transforms.Compose([\n",
        "            ToPILImage()])\n",
        "output=F.sigmoid(output)\n",
        "mask =output_transforms(output.squeeze(0).cpu())  # Remove batch dim\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(nrows=2,ncols=2, sharex='all', sharey='all', figsize=( 10,  10))\n",
        "\n",
        "# axs[0].imshow(x, origin='upper', interpolation=interp)\n",
        "\n",
        "# axs[1].set_title('blue should be down')\n",
        "# axs[1].imshow(x, origin='lower', interpolation=interp)\n",
        "#plt.show()\n",
        "\n",
        "\n",
        "#plt.figure(figsize=(15,10))\n",
        "#plt.subplot(2,2,1)\n",
        "#plt.imshow(output_transforms(test_single))\n",
        "axs[0,0].imshow(output_transforms(test_single))\n",
        "#plt.title('input test image')\n",
        "\n",
        "#plt.subplot(2,2,2)\n",
        "#plt.imshow(mask,cmap='gray')\n",
        "axs[0,1].imshow(mask,cmap='gray')\n",
        "#plt.title('output mask')\n",
        "\n",
        "test_single=cv2.imread('/content/kaggle_neuclei/stage2_test_final/00b4b9c026cfc91af21691503bad67081fc02d5c77b3cd27200d02dca6966b83/images/00b4b9c026cfc91af21691503bad67081fc02d5c77b3cd27200d02dca6966b83.png')\n",
        "trans_test=transforms.Compose([\n",
        "            Rescale(256),\n",
        "            ToTensor()])\n",
        "test_single=trans_test(test_single)\n",
        "\n",
        "output = model(test_single.unsqueeze(0).cuda())  # Add batch dim\n",
        "output_transforms = transforms.Compose([\n",
        "            ToPILImage()])\n",
        "output=F.sigmoid(output)\n",
        "mask =output_transforms(output.squeeze(0).cpu())  # Remove batch dim\n",
        "\n",
        "#plt.subplot(2,2,3)\n",
        "#plt.imshow(output_transforms(test_single))\n",
        "axs[1,0].imshow(output_transforms(test_single))\n",
        "#plt.title('input test image')\n",
        "\n",
        "#plt.subplot(2,2,4)\n",
        "#plt.imshow(mask,cmap='gray')\n",
        "#plt.title('output mask')\n",
        "axs[1,1].imshow(mask,cmap='gray')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZewnwqCSyzZV",
        "colab_type": "text"
      },
      "source": [
        "# Saving the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZW8C46IcCYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pickle.dump(model, open('sem_seg_ode_model_neuclie', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPaTCDxt6fuU",
        "colab_type": "text"
      },
      "source": [
        "# Brain Tumor Segmentation Dataset Downloading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "144AsS_06mDn",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "b225284d-4a78-4b31-b5c5-2b985eba566a"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "!kaggle datasets download -d mateuszbuda/lgg-mri-segmentation\n",
        "\n",
        "!unzip -q '/content/lgg-mri-segmentation.zip' #-d '/content/brain_tumor_segmentation'\n",
        "!rm  -r '/content/lgg-mri-segmentation.zip'\n",
        "!rm  -r '/content/lgg-mri-segmentation'\n",
        "!rm '/content/kaggle_3m/data.csv'\n",
        "!rm '/content/kaggle_3m/README.md'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNTaGMJ_-QA-",
        "colab_type": "text"
      },
      "source": [
        "# Dataloader Brain Tumor Segmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xn3VlBF_91Uu",
        "colab_type": "code",
        "outputId": "64d0375f-22cb-4813-b153-599a5ab325a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "source": [
        "train_img=cv2.cvtColor(cv2.imread('/content/kaggle_3m/TCGA_CS_4941_19960909/TCGA_CS_4941_19960909_11.tif'),cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(train_img)\n",
        "plt.title('original image')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "mask_image=cv2.cvtColor(cv2.imread('/content/kaggle_3m/TCGA_CS_4941_19960909/TCGA_CS_4941_19960909_11_mask.tif'),cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(mask_image)\n",
        "plt.title('mask image')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIHwKh2gEVX7",
        "colab_type": "code",
        "outputId": "a9742b48-fa53-4d11-a3f9-0af466b1b36a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_ids=[]\n",
        "\n",
        "for p in os.listdir('/content/kaggle_3m/'):\n",
        "  for c in os.listdir('/content/kaggle_3m/'+p):\n",
        "    #print(c)\n",
        "    pth=\"/content/kaggle_3m/\"+p+\"/\"+c\n",
        "    #print(pth)\n",
        "    if \"mask\" not in pth:\n",
        "      #print(pth)\n",
        "      train_ids.append(pth)\n",
        "\n",
        "#print(len(train_ids))\n",
        "train_ids=train_ids[:int(len(train_ids)*.20)]\n",
        "train_ids,val_ids=train_test_split(train_ids,test_size=.2,random_state=42)\n",
        "print(len(train_ids))\n",
        "print(len(val_ids))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCiKcMWbBiRO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BrainTumorDataset(Dataset):\n",
        "    def __init__(self, img_ids,im_shape, transform=None,mask_transform=None):\n",
        "        self.transform = transform\n",
        "        self.img_ids=img_ids\n",
        "        self.im_shape=im_shape\n",
        "        self.mask_transform=mask_transform\n",
        "\n",
        "    def __len__(self):      \n",
        "        return len(self.img_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #print(self.img_ids[idx])      \n",
        "        image = cv2.imread(self.img_ids[idx])\n",
        "        #print(image)\n",
        "        #target_img_pth=img_ids[idx][:-4]+'_mask.tif'\n",
        "        target_img = cv2.imread(self.img_ids[idx][:-4]+'_mask.tif',cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        if self.transform:\n",
        "          image=self.transform(image)\n",
        "        if self.mask_transform:\n",
        "          target_img=self.mask_transform(target_img)\n",
        "\n",
        "        return [image, target_img]\n",
        "\n",
        "\n",
        "trans = transforms.Compose([\n",
        "    Rescale(256),\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # imagenet\n",
        "])\n",
        "\n",
        "\n",
        "train_set = BrainTumorDataset(train_ids,256,trans,trans)\n",
        "val_set = BrainTumorDataset(val_ids,256,trans,trans)\n",
        "\n",
        "image_datasets = {\n",
        "    'train': train_set, 'val': val_set\n",
        "}\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "brain_tumor_dataloader = {\n",
        "    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n",
        "    'val': DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbxLwI2PNYMI",
        "colab_type": "code",
        "outputId": "e3daa036-538b-41d9-d724-1782824a8cd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        }
      },
      "source": [
        "image,mask=next(iter(brain_tumor_dataloader['val']))\n",
        "print(image.shape,mask.shape)\n",
        "from torchvision.transforms import transforms, ToTensor, ToPILImage\n",
        "def reverse_transform(inp,mask):   \n",
        "    output_transforms = transforms.Compose([\n",
        "            ToPILImage()])\n",
        "    mask=output_transforms(mask) \n",
        "    inp=output_transforms(inp)\n",
        "   \n",
        "    return inp,mask\n",
        "\n",
        "image,mask=reverse_transform(image[0],mask[0])\n",
        "plt.imshow(image)\n",
        "plt.title('input image')\n",
        "plt.figure()\n",
        "plt.imshow(mask,cmap='gray')\n",
        "plt.title('target mask')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Azo0VwpxQbME",
        "colab_type": "text"
      },
      "source": [
        "# Ode model for brain tumor segmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7o8Eizh3P3cV",
        "colab_type": "code",
        "outputId": "8ab5e829-388f-4d39-9817-f178a65d2dc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = SEM_SEG_ODE(ode,n_class=1,input_shape=(256,256,3))\n",
        "if use_cuda:\n",
        "    model = model.cuda()\n",
        "\n",
        "summary(model, (3, 256, 256))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xG0-TRqQ8IM",
        "colab_type": "text"
      },
      "source": [
        "# Defining The metrics function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLampElOQ_j5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "import torch.nn.functional as F\n",
        "from loss import dice_loss\n",
        "ce=nn.CrossEntropyLoss()\n",
        "def dice_loss1(input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Soft dice loss = 2*|A∩B| / |A|+|B|\n",
        "    \"\"\"\n",
        "    numerator = 2 * (input * target).sum((1, 2, 3))\n",
        "    denominator = (input + target).sum((1, 2, 3))\n",
        "\n",
        "    return (1 - numerator / denominator).mean()\n",
        "\n",
        "def dice_cof(input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Soft dice loss = 2*|A∩B| / |A|+|B|\n",
        "    \"\"\"\n",
        "    numerator = 2 * (input * target).sum((1, 2, 3))\n",
        "    denominator = (input + target).sum((1, 2, 3))\n",
        "\n",
        "    return ( numerator / denominator).mean()\n",
        "\n",
        "\n",
        "    \n",
        "def calc_metrics(pred, target, metrics, bce_weight=0.5):\n",
        "    bce = F.binary_cross_entropy_with_logits(pred, target)\n",
        "\n",
        "    pred = F.sigmoid(pred)\n",
        "    dice = dice_loss1(pred, target)\n",
        "    dice_co=dice_cof(pred,target)\n",
        "\n",
        "    loss = bce  + dice #* (1 - bce_weight)\n",
        "\n",
        "    numerator =  (pred * target).sum((1, 2, 3))\n",
        "    denominator = (pred + target).sum((1, 2, 3))\n",
        "\n",
        "    iou=(numerator / (denominator-numerator)).mean()\n",
        "\n",
        "\n",
        "    metrics['bce'] += bce.data.cpu().numpy() * target.size(0)\n",
        "    metrics['dice'] += dice.data.cpu().numpy() * target.size(0)\n",
        "    metrics['loss'] += loss.data.cpu().numpy() * target.size(0)\n",
        "    metrics['iou'] +=iou.data.cpu().numpy() * target.size(0)\n",
        "    metrics['dice_co']+=dice_co.data.cpu().numpy() * target.size(0)\n",
        "\n",
        "\n",
        "    return loss\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_F5ZEzKQuyA",
        "colab_type": "text"
      },
      "source": [
        "# Defnining Training function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zqVzZCXQ2bk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def print_metrics(metrics, epoch_samples, phase):\n",
        "    outputs = []\n",
        "    for k in metrics.keys():\n",
        "        outputs.append(\"{}: {:4f}\".format(k, metrics[k] / epoch_samples))\n",
        "\n",
        "    print(\"{}: {}\".format(phase, \", \".join(outputs)))\n",
        "\n",
        "def train_model(model, optimizer, scheduler, num_epochs=25):\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = 1e10\n",
        "    best_iou=-1\n",
        "    best_dice=1e10\n",
        "    best_bce=1e10\n",
        "    best_dice_co=-1\n",
        "\n",
        "\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for_once=1\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        since = time.time()\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "                for param_group in optimizer.param_groups:\n",
        "                    print(\"LR\", param_group['lr'])\n",
        "\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            metrics = defaultdict(float)\n",
        "            epoch_samples = 0\n",
        "\n",
        "            for inputs, labels in brain_tumor_dataloader[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                #inputs=inputs.type(torch.cuda.FloatTensor)\n",
        "                #labels=labels.type(torch.cuda.FloatTensor)\n",
        "                #print(inputs.shape,labels.shape)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    # if for_once==1:\n",
        "\n",
        "                    #   img,segmented_mask=reverse_transform(inputs[0],outputs[0])\n",
        "                    #   img,mask=reverse_transform(inputs[0],labels[0])\n",
        "                    #   plt.imshow(img)\n",
        "                    #   plt.title('input image')\n",
        "                    #   plt.figure()\n",
        "                    #   plt.imshow(mask,cmap='gray')\n",
        "                    #   plt.title('target mask')\n",
        "\n",
        "                    #   plt.figure()\n",
        "                    #   plt.imshow(segmented_mask,cmap='gray')\n",
        "                    #   plt.title('predected mask')\n",
        "                    #   for_once=0\n",
        "\n",
        "                    #print(outputs.shape)\n",
        "                    #outputs=outputs.int()\n",
        "                    loss = calc_metrics(outputs, labels, metrics)\n",
        "\n",
        "                    #print(loss.item())\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                epoch_samples += inputs.size(0)\n",
        "\n",
        "            print_metrics(metrics, epoch_samples, phase)\n",
        "            epoch_loss = metrics['loss'] / epoch_samples\n",
        "            epoch_dice = metrics['dice'] / epoch_samples\n",
        "            epoch_bce = metrics['bce'] / epoch_samples\n",
        "            epoch_iou = metrics['iou'] / epoch_samples\n",
        "            epoch_dice_co=metrics['dice_co'] / epoch_samples\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val':\n",
        "              if epoch_loss < best_loss:\n",
        "                print(\"saving best model\")\n",
        "                best_loss = epoch_loss\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "              if epoch_dice<best_dice:\n",
        "                best_dice=epoch_dice\n",
        "              if epoch_bce<best_bce:\n",
        "                best_bce=epoch_bce\n",
        "              if epoch_iou>best_iou:\n",
        "                best_iou=epoch_iou\n",
        "              if epoch_dice_co>best_dice_co:\n",
        "                best_dice_co=epoch_dice_co\n",
        "\n",
        "        time_elapsed = time.time() - since\n",
        "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "        #for_once=0\n",
        "\n",
        "    print('Best val loss: {:4f}'.format(best_loss))\n",
        "    print('Best val dice: {:4f}'.format(best_dice))\n",
        "    print('Best val bce: {:4f}'.format(best_bce))\n",
        "    print('Best val iou: {:4f}'.format(best_iou))\n",
        "    print('Best val dice_co: {:4f}'.format(best_dice_co))\n",
        "     # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPllYfwpQlKi",
        "colab_type": "text"
      },
      "source": [
        "# Training The ODE model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVs_lxfVQNw0",
        "colab_type": "code",
        "outputId": "61146edf-49db-495a-9c51-6623cc1dca7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import time\n",
        "import copy\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "optimizer_ft = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
        "\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=150, gamma=0.1)\n",
        "\n",
        "ode_brain_model = train_model(model, optimizer_ft, exp_lr_scheduler, num_epochs=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vT08I5ImS3xA",
        "colab_type": "text"
      },
      "source": [
        "# Defining and training the author's(buda) model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKh0aPxHS7tN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5083f102-2c5d-4ebc-cb45-ff769949eb53"
      },
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels=3, out_channels=1, init_features=32):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        features = init_features\n",
        "        self.encoder1 = UNet._block(in_channels, features, name=\"enc1\")\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder2 = UNet._block(features, features * 2, name=\"enc2\")\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder3 = UNet._block(features * 2, features * 4, name=\"enc3\")\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder4 = UNet._block(features * 4, features * 8, name=\"enc4\")\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.bottleneck = UNet._block(features * 8, features * 16, name=\"bottleneck\")\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose2d(\n",
        "            features * 16, features * 8, kernel_size=2, stride=2\n",
        "        )\n",
        "        self.decoder4 = UNet._block((features * 8) * 2, features * 8, name=\"dec4\")\n",
        "        self.upconv3 = nn.ConvTranspose2d(\n",
        "            features * 8, features * 4, kernel_size=2, stride=2\n",
        "        )\n",
        "        self.decoder3 = UNet._block((features * 4) * 2, features * 4, name=\"dec3\")\n",
        "        self.upconv2 = nn.ConvTranspose2d(\n",
        "            features * 4, features * 2, kernel_size=2, stride=2\n",
        "        )\n",
        "        self.decoder2 = UNet._block((features * 2) * 2, features * 2, name=\"dec2\")\n",
        "        self.upconv1 = nn.ConvTranspose2d(\n",
        "            features * 2, features, kernel_size=2, stride=2\n",
        "        )\n",
        "        self.decoder1 = UNet._block(features * 2, features, name=\"dec1\")\n",
        "\n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels=features, out_channels=out_channels, kernel_size=1\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc1 = self.encoder1(x)\n",
        "        enc2 = self.encoder2(self.pool1(enc1))\n",
        "        enc3 = self.encoder3(self.pool2(enc2))\n",
        "        enc4 = self.encoder4(self.pool3(enc3))\n",
        "\n",
        "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
        "\n",
        "        dec4 = self.upconv4(bottleneck)\n",
        "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
        "        dec4 = self.decoder4(dec4)\n",
        "        dec3 = self.upconv3(dec4)\n",
        "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
        "        dec3 = self.decoder3(dec3)\n",
        "        dec2 = self.upconv2(dec3)\n",
        "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
        "        dec2 = self.decoder2(dec2)\n",
        "        dec1 = self.upconv1(dec2)\n",
        "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
        "        dec1 = self.decoder1(dec1)\n",
        "        return self.conv(dec1)\n",
        "\n",
        "    @staticmethod\n",
        "    def _block(in_channels, features, name):\n",
        "        return nn.Sequential(\n",
        "            OrderedDict(\n",
        "                [\n",
        "                    (\n",
        "                        name + \"conv1\",\n",
        "                        nn.Conv2d(\n",
        "                            in_channels=in_channels,\n",
        "                            out_channels=features,\n",
        "                            kernel_size=3,\n",
        "                            padding=1,\n",
        "                            bias=False,\n",
        "                        ),\n",
        "                    ),\n",
        "                    (name + \"norm1\", nn.BatchNorm2d(num_features=features)),\n",
        "                    (name + \"relu1\", nn.ReLU(inplace=True)),\n",
        "                    (\n",
        "                        name + \"conv2\",\n",
        "                        nn.Conv2d(\n",
        "                            in_channels=features,\n",
        "                            out_channels=features,\n",
        "                            kernel_size=3,\n",
        "                            padding=1,\n",
        "                            bias=False,\n",
        "                        ),\n",
        "                    ),\n",
        "                    (name + \"norm2\", nn.BatchNorm2d(num_features=features)),\n",
        "                    (name + \"relu2\", nn.ReLU(inplace=True)),\n",
        "                ]\n",
        "            )\n",
        "        )\n",
        "authors_brain_model=UNet()\n",
        "\n",
        "if use_cuda:\n",
        "    authors_brain_model = authors_brain_model.cuda()\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "optimizer_ft = torch.optim.Adam(filter(lambda p: p.requires_grad, authors_brain_model.parameters()), lr=1e-3)\n",
        "\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=20, gamma=0.1)\n",
        "print(summary(authors_brain_model, (3, 256, 256)))\n",
        "authors_brain_model = train_model(authors_brain_model, optimizer_ft, exp_lr_scheduler, num_epochs=50)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3bMiD9F2nxd",
        "colab_type": "text"
      },
      "source": [
        "# training fcn unet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Apa4zbhT2rzU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5bab7c23-fdeb-481d-a7f1-4629559477c3"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "fcn_unet_brain = ResNetUNet(n_class=1)\n",
        "fcn_unet_brain = fcn_unet_brain.to(device)\n",
        "\n",
        "# check keras-like model summary using torchsummary\n",
        "from torchsummary import summary\n",
        "print(summary(fcn_unet_brain, input_size=(3, 192, 192)))\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import time\n",
        "import copy\n",
        "\n",
        "optimizer_ft = torch.optim.Adam(filter(lambda p: p.requires_grad, fcn_unet_brain.parameters()), lr=1e-3)\n",
        "\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=20, gamma=0.1)\n",
        "\n",
        "fcn_unet_brain = train_model(fcn_unet_brain, optimizer_ft, exp_lr_scheduler, num_epochs=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFubYr_WiJA1",
        "colab_type": "text"
      },
      "source": [
        "# Predicting on validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X99kUgk97GZL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=ode_brain_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3r_q2YIiIQF",
        "colab_type": "code",
        "outputId": "29d8effe-a1e4-44a9-cff4-2103be3398b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        }
      },
      "source": [
        "image,mask=next(iter(brain_tumor_dataloader['val']))\n",
        "image=image.to(device)\n",
        "#print(image[1].shape)\n",
        "mask=mask.to(device)\n",
        "model=model.eval()\n",
        "outputs=model(image)\n",
        "outputs=F.sigmoid(outputs)\n",
        "\n",
        "def reverse_transform(inp,mask):   \n",
        "    output_transforms = transforms.Compose([\n",
        "            ToPILImage()])\n",
        "    mask=mask.cpu()\n",
        "    inp=inp.cpu()\n",
        "    mask=output_transforms(mask) \n",
        "    inp=output_transforms(inp)\n",
        "   \n",
        "    return inp,mask\n",
        "fig, axs = plt.subplots(nrows=2,ncols=3, sharex='all', sharey='all', figsize=( 10,  6))\n",
        "\n",
        "img,segmented_mask=reverse_transform(image[0],outputs[0])\n",
        "img,msk=reverse_transform(image[0],mask[0])\n",
        "\n",
        "axs[0,0].imshow(img)\n",
        "#plt.title('input image')\n",
        "#plt.figure()\n",
        "axs[0,1].imshow(msk,cmap='gray')\n",
        "#plt.title('target mask')\n",
        "\n",
        "#plt.figure()\n",
        "axs[0,2].imshow(segmented_mask,cmap='gray')\n",
        "#plt.title('predected mask')\n",
        "\n",
        "img,segmented_mask=reverse_transform(image[1],outputs[1])\n",
        "#print(image[1].shape)\n",
        "img,msk=reverse_transform(image[1],mask[1])\n",
        "\n",
        "axs[1,0].imshow(img)\n",
        "#plt.title('input image')\n",
        "#plt.figure()\n",
        "axs[1,1].imshow(msk,cmap='gray')\n",
        "#plt.title('target mask')\n",
        "\n",
        "#plt.figure()\n",
        "axs[1,2].imshow(segmented_mask,cmap='gray')\n",
        "#plt.title('predected mask')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4qHiLvdQkk9",
        "colab_type": "text"
      },
      "source": []
    }
  ]
}